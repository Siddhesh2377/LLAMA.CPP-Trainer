{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# step1_load_base_model.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "model_name = \"LiquidAI/LFM2-350M\"  # Small model for testing\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! It has {base_model.num_parameters():,} parameters\")\n",
    "\n",
    "# Test it\n",
    "prompt = \"Write a Python function to add two numbers:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "outputs = base_model.generate(**inputs, max_length=100)\n",
    "print(\"\\nBase model output:\")\n",
    "print(tokenizer.decode(outputs, skip_special_tokens=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# step2_prepare_data.py\n",
    "from datasets import Dataset\n",
    "\n",
    "# Example: Train it to write Python code\n",
    "data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a function to reverse a string\",\n",
    "        \"output\": \"def reverse_string(s):\\n    return s[::-1]\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a function to check if a number is even\",\n",
    "        \"output\": \"def is_even(n):\\n    return n % 2 == 0\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a function to find the maximum in a list\",\n",
    "        \"output\": \"def find_max(lst):\\n    return max(lst)\"\n",
    "    },\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Format for training\n",
    "def format_instruction(sample):\n",
    "    return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "\n",
    "# Save it\n",
    "dataset.to_json(\"training_data.json\")\n",
    "print(f\"Created dataset with {len(dataset)} examples\")"
   ],
   "id": "6265d551fb06378b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about coding\",\n",
    "        \"output\": \"Silent loops awaken\\nLogic hums in midnight glow\\nBugs fade into dawn\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain recursion in simple terms\",\n",
    "        \"output\": \"Recursion is when a function solves a problem by calling itself on smaller versions of the same problem until it reaches a stopping condition.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short story about a lonely robot\",\n",
    "        \"output\": \"The robot powered on each night in the empty factory. With no humans left, it replayed old voice logs, pretending someone was still there to listen.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to open the camera app\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"open_app\\\", \\\"app_name\\\": \\\"camera\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to send a WhatsApp message to Rahul saying Hello\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"send_message\\\", \\\"app_name\\\": \\\"whatsapp\\\", \\\"contact_name\\\": \\\"Rahul\\\", \\\"message\\\": \\\"Hello\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize what an API is\",\n",
    "        \"output\": \"An API is a set of rules that allows different software systems to communicate and exchange data with each other.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a motivational quote about learning\",\n",
    "        \"output\": \"Learning is the quiet investment that compounds into confidence.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to enable WiFi\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"toggle_setting\\\", \\\"setting\\\": \\\"wifi\\\", \\\"state\\\": \\\"on\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what a database is\",\n",
    "        \"output\": \"A database is an organized collection of data that can be stored, managed, and retrieved efficiently.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a funny one-line joke about programmers\",\n",
    "        \"output\": \"Programmers donâ€™t get lost, they just take unexpected iterations.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to play music\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"play_music\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe artificial intelligence in one sentence\",\n",
    "        \"output\": \"Artificial intelligence is the simulation of human-like reasoning and learning in machines.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to set an alarm for 7 AM\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"set_alarm\\\", \\\"time\\\": \\\"07:00\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain machine learning briefly\",\n",
    "        \"output\": \"Machine learning is a method where computers learn patterns from data to make predictions or decisions without being explicitly programmed for each task.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a two-line poem about space\",\n",
    "        \"output\": \"Stars whisper ancient light,\\nTime drifts across the dark.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to open YouTube\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"open_app\\\", \\\"app_name\\\": \\\"youtube\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is cloud computing?\",\n",
    "        \"output\": \"Cloud computing is the delivery of computing services like storage, servers, and software over the internet instead of local machines.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to increase volume\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"adjust_volume\\\", \\\"direction\\\": \\\"up\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short description of blockchain\",\n",
    "        \"output\": \"Blockchain is a distributed ledger technology that records transactions securely across multiple computers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate JSON to turn on Bluetooth\",\n",
    "        \"output\": \"{\\\"action\\\": \\\"toggle_setting\\\", \\\"setting\\\": \\\"bluetooth\\\", \\\"state\\\": \\\"on\\\"}\"\n",
    "    }\n",
    "]\n",
    "dataset = Dataset.from_list(data)\n",
    "\n"
   ],
   "id": "8e88e0386062cb50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()  # Shows how few params we're training!"
   ],
   "id": "9d1c13e542f88dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"json\", data_files=\"training_data.json\", split=\"train\")\n",
    "\n",
    "def format_data(sample):\n",
    "    text = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_data)\n",
    "\n",
    "# 5. Tokenize\n",
    "def tokenize(sample):\n",
    "    result = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n"
   ],
   "id": "bdc02735410cf7f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_adapter_coding\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# 7. Train!\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# 8. Save the adapter\n",
    "model.save_pretrained(\"./lora_adapter_coding\")\n",
    "tokenizer.save_pretrained(\"./lora_adapter_coding\")\n",
    "print(\"LoRA adapter saved to ./lora_adapter_coding\")"
   ],
   "id": "2da787fe8a9be066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 6. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_adapter_creative\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# 7. Train!\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# 8. Save the adapter\n",
    "model.save_pretrained(\"./lora_adapter_creative\")\n",
    "tokenizer.save_pretrained(\"./lora_adapter_creative\")\n",
    "print(\"LoRA adapter saved to ./lora_adapter_creative\")"
   ],
   "id": "ab73ec3649dc3ffe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# step5_switch_adapters.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# 2. Load FIRST adapter (coding)\n",
    "print(\"=\" * 50)\n",
    "print(\"Loading CODING adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora_adapter_coding\")\n",
    "\n",
    "prompt = \"### Instruction:\\nWrite a function to calculate factorial\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_length=150, temperature=0.7)\n",
    "print(\"CODING ADAPTER OUTPUT:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 3. SWAP to second adapter (creative)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SWAPPING to CREATIVE adapter...\")\n",
    "model.load_adapter(\"./lora_adapter_creative\", adapter_name=\"creative\")\n",
    "model.set_adapter(\"creative\")\n",
    "\n",
    "prompt = \"### Instruction:\\nWrite a poem about AI\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_length=150, temperature=0.7)\n",
    "print(\"CREATIVE ADAPTER OUTPUT:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 4. Switch BACK to coding\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SWAPPING back to CODING adapter...\")\n",
    "model.set_adapter(\"default\")  # \"default\" is the first loaded adapter\n",
    "\n",
    "prompt = \"### Instruction:\\nWrite a function to sort a list\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_length=150, temperature=0.7)\n",
    "print(\"CODING ADAPTER OUTPUT (again):\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Notice: Base model stayed in memory the whole time!\")\n",
    "print(\"We just swapped tiny adapter files (~20MB each)\")"
   ],
   "id": "5e2aff746cf300fe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
